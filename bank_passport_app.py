# -*- coding: utf-8 -*-
import re
import io
import os
import time
from io import BytesIO
from typing import List, Dict
from datetime import datetime
from pathlib import Path
import pandas as pd
import streamlit as st

st.set_page_config(page_title="–ü–∞—Å–ø–æ—Ä—Ç –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏–π", layout="wide")

# Cache directory
CACHE_DIR = Path("cache")
CACHE_FILE = CACHE_DIR / "last_passport.xlsx"
CACHE_INFO = CACHE_DIR / "last_load_info.txt"

# ------------------------- Helpers -------------------------

def save_to_cache(uploaded_file):
    """Save uploaded file to cache"""
    CACHE_DIR.mkdir(exist_ok=True)
    
    # Read file content into memory first
    file_content = uploaded_file.getvalue()
    
    # Use temporary file to avoid permission issues
    temp_file = CACHE_DIR / "temp_passport.xlsx"
    temp_info = CACHE_DIR / "temp_load_info.txt"
    
    # Write to temp file first
    with open(temp_file, "wb") as f:
        f.write(file_content)
    
    # Save info to temp file
    load_time = datetime.now().strftime("%d.%m.%Y %H:%M:%S")
    with open(temp_info, "w", encoding="utf-8") as f:
        f.write(f"{load_time}\n{uploaded_file.name}")
    
    # Now replace the old files (Windows-safe way)
    time.sleep(0.2)  # Give time for any file handles to close
    
    # Remove old files if they exist
    for old_file in [CACHE_FILE, CACHE_INFO]:
        if old_file.exists():
            try:
                old_file.unlink()
            except PermissionError:
                time.sleep(0.3)
                try:
                    old_file.unlink()
                except:
                    pass  # If still can't delete, temp file will be used next time
    
    # Rename temp files to actual cache files
    try:
        temp_file.rename(CACHE_FILE)
        temp_info.rename(CACHE_INFO)
    except:
        pass  # Files will be picked up as temp files next time

def load_from_cache():
    """Load cached file if exists"""
    # Check for regular cache files first
    cache_file = CACHE_FILE
    info_file = CACHE_INFO
    
    # If regular files don't exist, check for temp files (from interrupted save)
    if not (cache_file.exists() and info_file.exists()):
        temp_cache = CACHE_DIR / "temp_passport.xlsx"
        temp_info = CACHE_DIR / "temp_load_info.txt"
        if temp_cache.exists() and temp_info.exists():
            cache_file = temp_cache
            info_file = temp_info
    
    if cache_file.exists() and info_file.exists():
        try:
            # Read info
            with open(info_file, "r", encoding="utf-8") as f:
                info = f.read().strip().split("\n")
                load_time = info[0] if len(info) > 0 else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                file_name = info[1] if len(info) > 1 else "—Ñ–∞–π–ª"
            
            # Read file into memory to avoid locking
            with open(cache_file, "rb") as f:
                file_bytes = f.read()
            
            # Load from BytesIO to avoid file locking
            df = pd.read_excel(BytesIO(file_bytes), sheet_name=0, header=0)
            df = df.dropna(axis=1, how="all")
            df.columns = [str(c).strip() for c in df.columns]
            
            return df, load_time, file_name
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞: {e}")
            return None, None, None
    return None, None, None

def load_excel(f) -> pd.DataFrame:
    try:
        if isinstance(f, (str, bytes)) or hasattr(f, "read"):
            df = pd.read_excel(f, sheet_name=0, header=0)
        else:
            df = pd.read_excel(f)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è Excel: {e}")
        raise
    # Drop fully empty columns
    df = df.dropna(axis=1, how="all")
    # Standardize column names (strip spaces, unify)
    df.columns = [str(c).strip() for c in df.columns]
    return df

def find_col(df: pd.DataFrame, patterns: List[str]) -> str | None:
    cols = list(df.columns)
    for p in patterns:
        regex = re.compile(p, re.IGNORECASE)
        for c in cols:
            if regex.search(str(c)):
                return c
    return None

def detect_law_mentions(df: pd.DataFrame) -> Dict[str, List[int]]:
    # Scan all textual columns for known law labels
    law_labels = ["44-–§–ó", "223-–§–ó", "185-–§–ó", "615 –ü–ü (–§–ö–†)", "275-–§–ó", "115-–§–ó", "289-–§–ó", "505 –ü–ü", "–ò–Ω—ã–µ"]
    law_map = {k: [] for k in law_labels}
    text_cols = [c for c in df.columns if df[c].dtype == "object"]
    
    # Map variations to canonical names
    law_variations = {
        "615 –ü–ü (–§–ö–†)": ["615", "615-–ø–ø", "–ø–ø 615", "–ø–ø —Ä—Ñ 615", "—Ñ–∫—Ä"],
        "44-–§–ó": ["44-—Ñ–∑", "44 —Ñ–∑"],
        "223-–§–ó": ["223-—Ñ–∑", "223 —Ñ–∑"],
        "185-–§–ó": ["185-—Ñ–∑", "185 —Ñ–∑"],
        "275-–§–ó": ["275-—Ñ–∑", "275 —Ñ–∑", "–∑–∞–∫—Ä—ã—Ç"],
        "115-–§–ó": ["115-—Ñ–∑", "115 —Ñ–∑", "–∫–æ–Ω—Ü–µ—Å—Å"],
        "289-–§–ó": ["289-—Ñ–∑", "289 —Ñ–∑", "—Ç–∞–º–æ–∂–µ–Ω"],
        "505 –ü–ü": ["505", "505-–ø–ø", "–ø–ø 505", "–∞–≤–∞–Ω—Å–∏—Ä"]
    }
    
    for idx, row in df.iterrows():
        cell_text = " | ".join(str(row[c]) for c in text_cols).lower()
        matched = False
        for law, variations in law_variations.items():
            if any(var in cell_text for var in variations):
                law_map[law].append(idx)
                matched = True
        if not matched:
            law_map["–ò–Ω—ã–µ"].append(idx)
    return law_map

def extract_stop_cols(df: pd.DataFrame) -> List[str]:
    # Columns containing STOP-like flags
    candidates = []
    for c in df.columns:
        cname = str(c).lower()
        if ("—Å—Ç–æ–ø" in cname) or ("stop" in cname) or ("–∑–∞–ø—Ä–µ—Ç" in cname):
            candidates.append(c)
    # Also include typical boolean/flag columns with "—É—Å–ª–æ–≤"
    for c in df.columns:
        cname = str(c).lower()
        if "—É—Å–ª–æ–≤" in cname and df[c].nunique() <= 5:
            candidates.append(c)
    # Deduplicate preserving order
    seen = set()
    out = []
    for c in candidates:
        if c not in seen:
            seen.add(c)
            out.append(c)
    return out

def extract_bg_type_cols(df: pd.DataFrame) -> List[str]:
    """Extract columns related to BG types"""
    candidates = []
    for c in df.columns:
        cname = str(c).lower()
        if any(x in cname for x in ["—É—á–∞—Å—Ç–∏–µ", "–∏—Å–ø–æ–ª–Ω–µ–Ω", "–≥–∞—Ä–∞–Ω—Ç", "–≤–æ–∑–≤—Ä–∞—Ç", "–Ω–∞–ª–æ–≥", "–Ω–¥—Å", "–ø–ª–∞—Ç–µ–∂", "–∞—Ä–µ–Ω–¥", "–∫–æ–º–º–µ—Ä—á", "–æ—Ñ—Ñ—Å–µ—Ç"]):
            if "—Å—Ç–æ–ø" not in cname and c in df.columns:
                candidates.append(c)
    # Deduplicate
    seen = set()
    out = []
    for c in candidates:
        if c not in seen and c in df.columns:
            seen.add(c)
            out.append(c)
    return out

def extract_additional_condition_cols(df: pd.DataFrame) -> List[str]:
    """Extract columns for additional conditions"""
    candidates = []
    for c in df.columns:
        cname = str(c).lower()
        # Add columns for special conditions
        if any(x in cname for x in ["–º—É–ª—å—Ç–∏–ª–æ—Ç", "–≤–∞–ª—é—Ç", "–∞–≤—Ç–æ–æ–¥–æ–±—Ä", "–ø–µ—Ä–µ–æ–±–µ—Å–ø–µ—á", "–∑–∞–∫—Ä—ã—Ç", "–∫–æ–Ω—Ü–µ—Å—Å", "—Ç–∞–º–æ–∂–µ–Ω"]):
            if "—Å—Ç–æ–ø" not in cname and c in df.columns:
                candidates.append(c)
    # Deduplicate
    seen = set()
    out = []
    for c in candidates:
        if c not in seen and c in df.columns:
            seen.add(c)
            out.append(c)
    return out

def normalize_bool_series(s: pd.Series) -> pd.Series:
    # Bring different truthy values to booleans
    def conv(x):
        if pd.isna(x):
            return False
        t = str(x).strip().lower()
        return t in {"1","true","–∏—Å—Ç–∏–Ω–∞","–¥–∞","yes","y","–¥","‚úì","‚úî","ok","–µ—Å—Ç—å","–≤–∫–ª","on"}
    return s.apply(conv)

def filter_by_keyword(df: pd.DataFrame, query: str) -> pd.DataFrame:
    if not query:
        return df
    pattern = re.escape(query.strip())
    mask = False
    for c in df.columns:
        ser = df[c]
        if ser.dtype == "object":
            mask = mask | ser.astype(str).str.contains(pattern, case=False, na=False, regex=True)
    return df[mask]

def to_excel_bytes(df: pd.DataFrame) -> bytes:
    output = BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="–§–∏–ª—å—Ç—Ä")
    return output.getvalue()

def extract_urls(text: str) -> List[str]:
    """Extract and clean URLs from text that may contain multiple URLs"""
    if not isinstance(text, str) or not text.strip():
        return []
    
    # Pattern to find URLs (http:// or https://)
    url_pattern = r'https?://[^\s<>"\'\)]*'
    urls = re.findall(url_pattern, text)
    
    # Clean and validate URLs
    cleaned = []
    for url in urls:
        url = url.strip()
        # Remove common trailing punctuation
        url = url.rstrip('.,;:)')
        if url and len(url) > 10:  # Basic validation
            cleaned.append(url)
    
    return cleaned

# ------------------------- UI -------------------------

st.title("–ü–∞—Å–ø–æ—Ä—Ç –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏–π")

with st.sidebar:
    st.header("–§–∏–ª—å—Ç—Ä—ã")

    # Data source
    st.markdown("### üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    
    # Try to load from cache first
    cached_df, load_time, file_name = load_from_cache()
    
    if cached_df is not None:
        st.success(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω: **{file_name}**")
        st.caption(f"–î–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {load_time}")
        
        # Option to clear cache
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ –û—á–∏—Å—Ç–∏—Ç—å", width="stretch"):
                # Remove all cache files (including temp files)
                cache_files = [
                    CACHE_FILE, 
                    CACHE_INFO,
                    CACHE_DIR / "temp_passport.xlsx",
                    CACHE_DIR / "temp_load_info.txt"
                ]
                for f in cache_files:
                    if f.exists():
                        try:
                            f.unlink()
                        except:
                            pass
                st.rerun()
        
        df = cached_df
    else:
        st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª Excel")
        df = None
    
    # File uploader (always visible)
    uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–π —Ñ–∞–π–ª", type=["xlsx"], label_visibility="collapsed")
    
    if uploaded:
        # Load directly from uploaded file
        try:
            df = load_excel(uploaded)
            st.success(f"‚úÖ **{uploaded.name}** –∑–∞–≥—Ä—É–∂–µ–Ω!")
            # Save to cache in background (non-blocking)
            try:
                save_to_cache(uploaded)
                st.info("üíæ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –∫–µ—à –∏ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—É—Å–∫–µ")
            except Exception as cache_error:
                st.warning(f"‚ö†Ô∏è –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω, –Ω–æ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –∫–µ—à: {cache_error}")
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
            df = None

if df is not None and len(df):
    # Column detection
    bank_col = find_col(df, [r"^–±–∞–Ω–∫", r"–Ω–∞–∑–≤.*–±–∞–Ω–∫", r"–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ.*–±–∞–Ω–∫", r"–±–∞–Ω–∫$"])
    lim_deal_col = find_col(df, [r"–ª–∏–º–∏—Ç.*—Å–¥–µ–ª–∫", r"–Ω–∞\s*—Å–¥–µ–ª–∫", r"deal", r"per\s*deal", r"–ª–∏–º–∏—Ç.*–æ–ø–µ—Ä–∞—Ü"])
    lim_client_col = find_col(df, [r"–ª–∏–º–∏—Ç.*–∫–ª–∏–µ–Ω—Ç", r"–Ω–∞\s*–∫–ª–∏–µ–Ω—Ç", r"per\s*client"])
    url_col = find_col(df, [r"—Å—Å—ã–ª", r"url", r"–∑–∞—è–≤", r"application", r"link"])
    extra_req_col = find_col(df, [r"–¥–æ–ø(\.|–æ–ª–Ω–∏—Ç–µ–ª—å–Ω).*—Ç—Ä–µ–±", r"—Ç—Ä–µ–±–æ–≤", r"–∫–æ–º–º–µ–Ω—Ç–∞—Ä"])

    # Sidebar controls
    with st.sidebar:
        # Basic filters
        st.markdown("### üîç –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
        if bank_col:
            bank_filter = st.text_input("–ë–∞–Ω–∫ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞)", "")
        else:
            bank_filter = ""
        
        # Numeric limit filters if available
        lim_deal_min = lim_deal_max = None
        lim_client_min = lim_client_max = None

        if lim_deal_col and pd.api.types.is_numeric_dtype(df[lim_deal_col]):
            lim_deal_min, lim_deal_max = st.slider(
                f"{lim_deal_col}",
                float(df[lim_deal_col].min(skipna=True)),
                float(df[lim_deal_col].max(skipna=True)),
                (float(df[lim_deal_col].min(skipna=True)), float(df[lim_deal_col].max(skipna=True)))
            )

        if lim_client_col and pd.api.types.is_numeric_dtype(df[lim_client_col]):
            lim_client_min, lim_client_max = st.slider(
                f"{lim_client_col}",
                float(df[lim_client_col].min(skipna=True)),
                float(df[lim_client_col].max(skipna=True)),
                (float(df[lim_client_col].min(skipna=True)), float(df[lim_client_col].max(skipna=True)))
            )

        st.divider()

        # Laws
        st.markdown("### üìú –ó–∞–∫–æ–Ω—ã")
        st.caption("–ü–æ–∫–∞–∂–µ—Ç –±–∞–Ω–∫–∏ —Å –õ–Æ–ë–´–ú –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∑–∞–∫–æ–Ω–æ–≤:")
        law_map = detect_law_mentions(df)
        law_choices = [l for l in law_map.keys() if l != "–ò–Ω—ã–µ"]
        selected_laws = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–∫–æ–Ω—ã", options=law_choices, default=[])

        st.divider()

        # BG Types
        st.markdown("### üè¶ –¢–∏–ø—ã –ë–ì")
        st.caption("–ü–æ–∫–∞–∂–µ—Ç –±–∞–Ω–∫–∏ —Å –õ–Æ–ë–´–ú –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤:")
        bg_cols = extract_bg_type_cols(df)
        selected_bg_cols = []
        if bg_cols:
            for c in bg_cols:
                if st.checkbox(c, value=False, key=f"bg_{c}"):
                    selected_bg_cols.append(c)

        st.divider()

        # Additional conditions
        st.markdown("### ‚öôÔ∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è")
        st.caption("–ü–æ–∫–∞–∂–µ—Ç –±–∞–Ω–∫–∏ —Å –õ–Æ–ë–´–ú –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π:")
        add_cond_cols = extract_additional_condition_cols(df)
        selected_add_cond_cols = []
        if add_cond_cols:
            for c in add_cond_cols:
                if st.checkbox(c, value=False, key=f"cond_{c}"):
                    selected_add_cond_cols.append(c)

        st.divider()

        # STOP
        st.markdown("### ‚õî –°–¢–û–ü-—É—Å–ª–æ–≤–∏—è (–∏—Å–∫–ª—é—á–µ–Ω–∏—è)")
        st.caption("–ü–æ–∫–∞–∂–µ—Ç –±–∞–Ω–∫–∏ –ë–ï–ó –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –°–¢–û–ü-—É—Å–ª–æ–≤–∏–π (—Ç.–µ. –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –±–∞–Ω–∫–∏):")
        stop_cols = extract_stop_cols(df)
        selected_stop_cols = []
        for c in stop_cols:
            if st.checkbox(c, value=False, key=f"stop_{c}"):
                selected_stop_cols.append(c)

        st.divider()

        # Additional requirements
        st.markdown("### üìù –ü–æ–∏—Å–∫")
        extra_query = st.text_input("–î–æ–ø. —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞)", "")
        q = st.text_input("–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º", "")

    # Apply filters
    filtered = df.copy()

    # Bank filter
    if bank_col and bank_filter.strip():
        filtered = filtered[filtered[bank_col].astype(str).str.contains(bank_filter.strip(), case=False, na=False)]

    # Limit filters
    if lim_deal_col and lim_deal_min is not None:
        filtered = filtered[filtered[lim_deal_col].between(lim_deal_min, lim_deal_max, inclusive="both")]
    if lim_client_col and lim_client_min is not None:
        filtered = filtered[filtered[lim_client_col].between(lim_client_min, lim_client_max, inclusive="both")]

    # Law filters (OR logic - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –±–∞–Ω–∫–∏ —Å –õ–Æ–ë–´–ú –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∑–∞–∫–æ–Ω–æ–≤)
    if selected_laws:
        idx_sets = [set(law_map.get(law, [])) for law in selected_laws]
        if idx_sets:
            keep_idx = set.union(*idx_sets) if idx_sets else set()
            filtered = filtered.loc[filtered.index.intersection(list(keep_idx))]

    # BG type filters (OR logic - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –±–∞–Ω–∫–∏ —Å –õ–Æ–ë–´–ú –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ë–ì)
    if selected_bg_cols:
        mask = pd.Series([False] * len(filtered), index=filtered.index)
        for c in selected_bg_cols:
            if c in filtered.columns:
                mask = mask | normalize_bool_series(filtered[c])
        filtered = filtered[mask]

    # Additional condition filters (OR logic - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –±–∞–Ω–∫–∏ —Å –õ–Æ–ë–´–ú –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π)
    if selected_add_cond_cols:
        mask = pd.Series([False] * len(filtered), index=filtered.index)
        for c in selected_add_cond_cols:
            if c in filtered.columns:
                mask = mask | normalize_bool_series(filtered[c])
        filtered = filtered[mask]

    # STOP condition filters (AND logic - –∏—Å–∫–ª—é—á–∞–µ–º –±–∞–Ω–∫–∏ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –°–¢–û–ü-—É—Å–ª–æ–≤–∏—è–º–∏)
    # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–æ "–°–¢–û–ü —Ä–µ–≥–∏–æ–Ω—ã", –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±–∞–Ω–∫–∏ –ë–ï–ó —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
    for c in selected_stop_cols:
        if c in filtered.columns:
            # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ª–æ–≥–∏–∫—É: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±–∞–Ω–∫–∏ –≥–¥–µ –°–¢–û–ü = False (–Ω–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
            filtered = filtered[~normalize_bool_series(filtered[c])]

    # Text search filters
    if extra_req_col and extra_query.strip():
        filtered = filtered[filtered[extra_req_col].astype(str).str.contains(re.escape(extra_query.strip()), case=False, na=False)]

    if q.strip():
        filtered = filter_by_keyword(filtered, q.strip())

    # Bank detail picker
    with st.container():
        col1, col2 = st.columns([3, 1])
        with col1:
            st.subheader("–¢–∞–±–ª–∏—Ü–∞")
        with col2:
            st.metric("–ù–∞–π–¥–µ–Ω–æ –±–∞–Ω–∫–æ–≤", len(filtered))
        
        # Build a compact view with optional '–ü–æ–¥—Ä–æ–±–Ω–µ–µ'
        view = filtered.copy()
        if bank_col and "–ü–æ–¥—Ä–æ–±–Ω–µ–µ" not in view.columns:
            view.insert(0, "–ü–æ–¥—Ä–æ–±–Ω–µ–µ", [f"–û—Ç–∫—Ä—ã—Ç—å:{i}" for i in view.index])

        st.dataframe(
            view,
            width="stretch",
            hide_index=True
        )

        # Bank selector
        bank_options = []
        if bank_col:
            bank_options = filtered[bank_col].dropna().astype(str).unique().tolist()
            bank_options = sorted(bank_options, key=lambda x: x.lower())

        st.divider()
        selected_bank = st.selectbox("–û—Ç–∫—Ä—ã—Ç—å –∫–∞—Ä—Ç–æ—á–∫—É –±–∞–Ω–∫–∞", options=["‚Äî"] + bank_options, index=0)

        if selected_bank != "‚Äî":
            row = filtered[filtered[bank_col].astype(str) == selected_bank].head(1)
            if not row.empty:
                r = row.iloc[0].to_dict()
                with st.expander(f"üìã –ö–∞—Ä—Ç–æ—á–∫–∞ –±–∞–Ω–∫–∞: {selected_bank}", expanded=True):
                    st.markdown(f"### {selected_bank}")
                    
                    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    st.markdown("#### üìä –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
                    cols = st.columns(3)
                    with cols[0]:
                        if lim_deal_col:
                            st.metric("–õ–∏–º–∏—Ç –Ω–∞ —Å–¥–µ–ª–∫—É", r.get(lim_deal_col, '‚Äî'))
                    with cols[1]:
                        if lim_client_col:
                            st.metric("–õ–∏–º–∏—Ç –Ω–∞ –∫–ª–∏–µ–Ω—Ç–∞", r.get(lim_client_col, '‚Äî'))
                    with cols[2]:
                        max_term_col = find_col(df, [r"–º–∞–∫—Å–∏–º–∞–ª—å–Ω.*—Å—Ä–æ–∫", r"—Å—Ä–æ–∫.*–±–≥", r"max.*term"])
                        if max_term_col:
                            st.metric("–ú–∞–∫—Å. —Å—Ä–æ–∫ –ë–ì", r.get(max_term_col, '‚Äî'))
                    
                    st.divider()
                    
                    # –ó–∞–∫–æ–Ω—ã –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã
                    st.markdown("#### üìú –ó–∞–∫–æ–Ω—ã –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã")
                    law_cols = []
                    for c in df.columns:
                        cname = str(c).lower()
                        if any(x in cname for x in ["—Ñ–∑", "–ø–ø", "–∑–∞–∫–æ–Ω", "–∫–æ–Ω—Ü–µ—Å—Å", "–∑–∞–∫—É–ø–∫", "—Ç–∞–º–æ–∂–µ–Ω"]):
                            if "—Å—Ç–æ–ø" not in cname:
                                law_cols.append(c)
                    
                    if law_cols:
                        law_col_chunks = [law_cols[i:i+4] for i in range(0, len(law_cols), 4)]
                        for chunk in law_col_chunks:
                            cols = st.columns(len(chunk))
                            for i, c in enumerate(chunk):
                                with cols[i]:
                                    val = r.get(c, '')
                                    is_yes = normalize_bool_series(pd.Series([val])).iloc[0]
                                    icon = "‚úÖ" if is_yes else "‚ùå"
                                    st.write(f"{icon} **{c}**")
                    
                    st.divider()
                    
                    # –¢–∏–ø—ã –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –≥–∞—Ä–∞–Ω—Ç–∏–π
                    st.markdown("#### üè¶ –¢–∏–ø—ã –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –≥–∞—Ä–∞–Ω—Ç–∏–π")
                    bg_cols = []
                    for c in df.columns:
                        cname = str(c).lower()
                        if any(x in cname for x in ["—É—á–∞—Å—Ç–∏–µ", "–∏—Å–ø–æ–ª–Ω–µ–Ω", "–≥–∞—Ä–∞–Ω—Ç", "–≤–æ–∑–≤—Ä–∞—Ç", "–Ω–∞–ª–æ–≥", "–Ω–¥—Å", "–ø–ª–∞—Ç–µ–∂", "–∞—Ä–µ–Ω–¥", "–∫–æ–º–º–µ—Ä—á", "–æ—Ñ—Ñ—Å–µ—Ç"]):
                            if "—Å—Ç–æ–ø" not in cname and c not in law_cols:
                                bg_cols.append(c)
                    
                    if bg_cols:
                        bg_col_chunks = [bg_cols[i:i+4] for i in range(0, len(bg_cols), 4)]
                        for chunk in bg_col_chunks:
                            cols = st.columns(len(chunk))
                            for i, c in enumerate(chunk):
                                with cols[i]:
                                    val = r.get(c, '')
                                    is_yes = normalize_bool_series(pd.Series([val])).iloc[0]
                                    icon = "‚úÖ" if is_yes else "‚ùå"
                                    st.write(f"{icon} **{c}**")
                    
                    st.divider()
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
                    st.markdown("#### ‚öôÔ∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è")
                    cond_cols = []
                    for c in df.columns:
                        cname = str(c).lower()
                        if any(x in cname for x in ["–º—É–ª—å—Ç–∏–ª–æ—Ç", "–≤–∞–ª—é—Ç", "–∞–≤—Ç–æ–æ–¥–æ–±—Ä", "–ø–µ—Ä–µ–æ–±–µ—Å–ø–µ—á", "–¥–æ—Å—Ç–∞–≤–∫", "—Å–∞–º–æ–≤—ã–≤", "–æ–ø–ª–∞—Ç–∞"]):
                            if "—Å—Ç–æ–ø" not in cname:
                                cond_cols.append(c)
                    
                    if cond_cols:
                        cond_col_chunks = [cond_cols[i:i+4] for i in range(0, len(cond_cols), 4)]
                        for chunk in cond_col_chunks:
                            cols = st.columns(len(chunk))
                            for i, c in enumerate(chunk):
                                with cols[i]:
                                    val = r.get(c, '')
                                    is_yes = normalize_bool_series(pd.Series([val])).iloc[0]
                                    icon = "‚úÖ" if is_yes else "‚ùå"
                                    st.write(f"{icon} **{c}**")
                    
                    st.divider()
                    
                    # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã–µ —Ñ–æ—Ä–º—ã
                    st.markdown("#### üè¢ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã–µ —Ñ–æ—Ä–º—ã")
                    opf_cols = []
                    for c in df.columns:
                        cname = str(c).lower()
                        if any(x in cname for x in ["–æ–æ–æ", "–æ–∞–æ", "–∑–∞–æ", "–ø–∞–æ", "—Ñ–≥—É–ø", "–º—É–ø", "–≥—É–ø", "–∞–Ω–æ"]) or \
                           (("–∏–ø" in cname or "–æ–ø—Ñ" in cname or "—Ñ–æ—Ä–º" in cname) and "—Å—Ç–æ–ø" not in cname):
                            opf_cols.append(c)
                    
                    if opf_cols:
                        opf_col_chunks = [opf_cols[i:i+4] for i in range(0, len(opf_cols), 4)]
                        for chunk in opf_col_chunks:
                            cols = st.columns(len(chunk))
                            for i, c in enumerate(chunk):
                                with cols[i]:
                                    val = r.get(c, '')
                                    is_yes = normalize_bool_series(pd.Series([val])).iloc[0]
                                    icon = "‚úÖ" if is_yes else "‚ùå"
                                    st.write(f"{icon} **{c}**")
                    
                    st.divider()
                    
                    # –°–¢–û–ü-—É—Å–ª–æ–≤–∏—è
                    st.markdown("#### ‚õî –°–¢–û–ü-—É—Å–ª–æ–≤–∏—è")
                    if stop_cols:
                        items_yes = []
                        items_no = []
                        for c in stop_cols:
                            val = r.get(c, None)
                            truthy = normalize_bool_series(pd.Series([val])).iloc[0]
                            if truthy:
                                items_yes.append(f"üî¥ {c}")
                            else:
                                items_no.append(f"üü¢ {c}")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            if items_yes:
                                st.markdown("**‚ùå –ï—Å—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**")
                                st.markdown("\n".join(items_yes))
                            else:
                                st.success("‚úÖ –ù–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π!")
                        with col2:
                            if items_no:
                                st.markdown("**‚úÖ –ù–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π:**")
                                st.markdown("\n".join(items_no))
                    
                    st.divider()
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    st.markdown("#### üìù –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
                    
                    # –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
                    org_req_col = find_col(df, [r"—Å—Ä–æ–∫.*–¥–µ–π—Å—Ç–≤.*–æ—Ä–≥–∞–Ω", r"–≤–æ–∑—Ä–∞—Å—Ç.*–∫–æ–º–ø–∞–Ω"])
                    if org_req_col:
                        st.write(f"**{org_req_col}:** {r.get(org_req_col, '‚Äî')}")
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
                    if extra_req_col:
                        req_text = r.get(extra_req_col, '')
                        if req_text and str(req_text).strip() and str(req_text).lower() not in ['nan', 'none', '‚Äî']:
                            st.write(f"**–î–æ–ø. —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**")
                            st.info(req_text)
                    
                    # –§–∏—à–∫–∏ –±–∞–Ω–∫–∞
                    feat_col = find_col(df, [r"—Ñ–∏—à–∫", r"–æ—Å–æ–±–µ–Ω", r"–ø—Ä–µ–∏–º—É—â–µ—Å—Ç"])
                    if feat_col:
                        feat_text = r.get(feat_col, '')
                        if feat_text and str(feat_text).strip() and str(feat_text).lower() not in ['nan', 'none', '‚Äî']:
                            st.write(f"**–§–∏—à–∫–∏ –±–∞–Ω–∫–∞:**")
                            st.success(feat_text)
                    
                    # –°—Å—ã–ª–∫–∏
                    if url_col:
                        link_text = r.get(url_col, "")
                        urls = extract_urls(str(link_text))
                        if urls:
                            st.markdown("#### üîó –ü–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É")
                            for i, url in enumerate(urls, 1):
                                label = "–ü–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É" if len(urls) == 1 else f"–ü–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É #{i}"
                                st.link_button(label, url=url, width="stretch")

    # Export
    st.subheader("–≠–∫—Å–ø–æ—Ä—Ç")
    csv_bytes = filtered.to_csv(index=False).encode("utf-8-sig")
    xlsx_bytes = to_excel_bytes(filtered)

    st.download_button("–°–∫–∞—á–∞—Ç—å CSV", data=csv_bytes, file_name="—Ñ–∏–ª—å—Ç—Ä_–±–∞–Ω–∫–∏.csv", mime="text/csv")
    st.download_button("–°–∫–∞—á–∞—Ç—å Excel", data=xlsx_bytes, file_name="—Ñ–∏–ª—å—Ç—Ä_–±–∞–Ω–∫–∏.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

else:
    st.info("üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª Excel –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ —Å–ª–µ–≤–∞")
